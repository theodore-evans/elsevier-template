% Template as of 22.04.2021 error free
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,5p,review]{elsarticle}
%\documentclass[final,5p,times,twocolumn]{elsarticle}

\usepackage{lineno}
\usepackage{easylist}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage[breaklinks]{hyperref}
\usepackage{url}
\usepackage{textcomp}
\usepackage{verbatim}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{ulem}
%\usepackage[ampersand]{easylist}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{listings} 
\pgfplotsset{compat=1.14}
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}
\usepackage{tikzscale}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usepackage{tikz-qtree,tikz-qtree-compat}
\usetikzlibrary{calc}
\modulolinenumbers[5]

\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

\pdfstringdefDisableCommands{%
  \def\corref#1{}%
}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}

\title{Crisp, Short, Clear, Concise Title of the Paper}

%% or include affiliations in footnotes:
\author[TUB]{Christian Geissler}
\author[TUB]{Theodore Evans}
\author[CAR]{Norman Zerbe}
\author[xxx]{Erika Musterfrau}
\author[xxx]{Max Mustermann}
\author[MUG]{Markus Plass}
\author[MUG]{Heimo Mueller}
\author[MUG,amii]{Andreas Holzinger \corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{andreas.holzinger@medunigraz.at}

\address[TUB]{Technical University Berlin}
\address[MUG]{Medical University Graz, Austria}
\address[amii]{Alberta Machine Intelligence Institute, Canada}
\address[xxx]{Lab Name, University Name, Address}

\begin{abstract} 
What was already known on the topic to the international research community ?
%This must be shortened still
The spread of the use of artificial intelligence techniques is now ubiquitous and unstoppable. It brings many opportunities and helps solve old problems. However, by its very nature, the use of AI also brings many risks and new problems that must be addressed to avoid jeopardizing effective evolution. The emerging field of eXplainable AI (XAI) is helping to find answers to these problems and put people more at the center.
While from a research perspective, discussions of XAI date back several decades and were reinvigorated by the DARPA initiative, the concept emerged with renewed vigor in late 2019 when Google announced a new XAI toolset for developers after announcing its "AI-first" strategy in 2017. This is because many of today's machine and deep learning applications do not allow us to fully understand how they work or the logic behind them, which is referred to as a "black box." The high complexity makes many successful machine learning models difficult or impossible to understand. This characteristic is considered one of the biggest problems in the application of AI techniques; it makes machine decisions non-transparent and often incomprehensible even to experts or developers. Explainable AI systems can explain the logic of decisions, characterize the strengths and weaknesses of decision making, and provide insights into their future behavior.
What this paper contributes to the international research community ?

This paper describes, analyzes  ...

The novelty is, the results show, the paper demonstrates ...

The benefit is, it indicates that ...



%The spread of the use of artificial intelligence techniques is now pervasive and unstoppable. However, it brings with its opportunities but also risks and problems that must be addressed in order not to compromise an effective evolution. The eXplainable AI (XAI) is one of the answers to these problems to bring humans closer to machines. While from a research perspective the discussions on XAI date back a few decades, the concept emerged with renewed vigour at the end of 2019 when Google, after announcing its "AI-first" strategy in 2017, recently announced a new XAI toolset for developers. Nowadays many of the machine and deep learning applications do not allow you to understand how they work entirely or the logic behind them for effect called "BlackBox", according to which machine learning models are mostly black boxes. This feature is considered one of the biggest problems in the application of AI techniques; it makes machine decisions not transparent and often incomprehensible even to the eyes of experts or developers themselves. Explainable AI systems can explain the logic of decisions, characterize the strengths and weaknesses of decision making, and provide insights into their future behaviour.
\end{abstract}

\begin{keyword}
Explainable AI, interpretable Machine Learning, interactive Machine Learning, aggregation functions, ordinal sums, Glass--box approach, transparency 
\end{keyword}

\end{frontmatter}
\linenumbers

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OVERVIEW

% Motivation
% - Explainability as cross-disciplinary, embedded concept
% - Limitations of focusing on either algorithmic or UX aspects alone
% - Need for deeper, cross-disciplinary study of explainability requirements in a specific context

% Outcome
% - Exploratory case study 
% - Investigating interpretations, expectations and requirements 
% - For a set of Ki-67 AI solutions
% - For 3 selected ‘personae’ 

% Future work
% - Gaps between requirements and current SOA indicate fruitful directions
% - Can be referenced for those met by EMPAIA (e.g. standardisation, unified interface, etc.)

% Next steps
% - Related work in progress?
% - Design case study
% - Identify personae (and corresponding stakeholders)
% - Identify solution developers

% Potential issues
% - Not enough time to conduct data collection + analysis
% - Not enough market penetration of chosen candidates for Ki-67 solutions, how to gain user insights into unfamiliar solution/workflow?
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{\textbf{Highlights}}

\begin{itemize}
    \item This paper describes, analyzes, 
    \item The novelty is, the results show, the paper demonstrates
    \item The benefit is, it indicates that, 
\end{itemize}

\section{Introduction}
\label{sec:Introduction}

A reference looks like this \cite{negahbani2020pathonet}

%% INTRODUCE PAPER
% Short preamble about 'black-box' methods in Medicine, focus on DL \cite{piccialli_survey_2021}

% Current regulatory landscape regarding explainability in AI-assisted digital pathology

% Another short preamble about explainability methods for medicine \cite{poceviciute_survey_2020} + more recent survey papers? What classes of XAI are available, in use > summary of the type of information these provide about models in AI solutions

% Address shortcoming identified  on the limitations of working on the topic of explainability from a purely algorithmic perspective. Need for embedded, transdisciplinary XAI research \cite{tjoa_survey_2020} (and others)

% Introduction of this paper as a case study in context of a (set of) specific application(s), and the interpretations, expectations and needs of specific stakeholders of this application.

% What identified requirements can already be met by the current technical state of the art, what are the barriers that need to be overcome to connect user requirements with technical capabilities? What open research questions does this identify

%% INTRODUCE EMPAIA

\section{Case study design}
\label{sec:CaseStudyDesign}

% We followed the guidelines laid out in \cite{}

% \cite{chakraborti_emerging_2020} identifies the need for understanding the 'personae' of stakeholders

\subsection{Selection of AI solutions}

% Ki-67 app(s) selected based on xyz, 
% Possible criteria:
% - CE-IVD approved for clinical use
% - black-box algorithm (i.e. DL)
% - results not easily verifiable by clinician with cursory visual inspection

% Candidates:
% - Visiopharm Ki-67 app (https://visiopharm.com/app-center/app/ki-67-app-breast-cancer/, demo: https://www.labroots.com/ms/webinar/standardization-clinical-digital-pathology-ki-67)
% - Roche VENTANA Companion Algorithm Ki-67 (30-9) (https://diagnostics.roche.com/no/en/products/instruments/ventana-companion-algorithm-image-analysis-software.html), also has FDA 510(k) clearance
% - MindPeak BreastIHC (https://www.mindpeak.ai/products/mindpeak-breastihc) CE-IVD clearance pending

\subsection{Selection of stakeholders}

% User personae 1-3 based on xyz. /cite{poceviciute_survey_2020} Ongoing work by Graz? @Andreas
% Stakeholders selected from EMPAIA Gremien members

\subsection{Research questions}

% Research questions based on:
%  - requirement gathering workshop
%  - current SOA on medical XAI
%  - open questions in the technical literature
%  - decision-making processes in EMPAIA infrastructure design?

% Could be descriptions/demonstrations of classes of XAI given in {poceviciute_survey_2020}, plus additional options from the EMPAIA brainstorming session, with an ordinal scale indicating the degree to which this additional information:
%   - is intelligible to the user
%   - increases trust in the result
%   - is generalizable to other solutions?
% Could reference/directly use questions from the SCS in {HolzingerEtAl:2020:QualityOfExplanations} in the survey design
% Potential pitfall: implementation of classes of XAI on a model too time-consuming to manage before submission > mock up with description instead?

\subsection{Data collection protocol}
% Technical SOA: literature review protocol

% Case study:
% 0. Initial brainstorming session with a simple app
% 1. Wide-reaching survey to stakeholder groups (EMPAIA Gremien) according to self-identification to user personae
% 2. Focused interviews with volunteering stakeholders
% 3. Collection and analysis of data

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Case study 1: Solution developer}

\section{Case study 2: Clinical end user}

\section{Case study 3: QA}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:Conclusion}

\section{Future work}
\label{sec:FutureWork}
\section*{Acknowledgements}

We are grateful for the support of Mister Helpful for helping in shuffling. All Authors declare that there are no conflicts of interests. This work does not raise any ethical issues. This work was done in the context of EMPAIA Germany. Parts of this work have received funding from the Austrian Research Promotion Agency (FFG) under grant agreement No. 879881 (EMPAIA) and by the Austrian Science Fund (FWF), Project: P-32554 explainable Artificial Intelligence. 

\bibliography{references}

% \section*{About the Authors}

% \parpic{\includegraphics[width=1in,clip,keepaspectratio]{bio-images/dummy.jpg}}
%\noindent {\bf Author Name} is the most famous author in her field. 
%She is particularly interested in X and Y, and also dabbles in Z.

% \bio{bio-images/evans.png}
% Theodore Evans received his M.Phys. degree in physics from the University of Manchester. He is a Ph.D. Candidate at the Distributed Artificial Intelligence Laboratory (DAI-Labor) of the Technisches Universität Berlin (TU-Berlin). He is currently supervised by Prof. Dr. Dr. hc Sahin Albayrak. His research interests lie in representation learning and cross-disciplinary approaches to explainable AI-assistance for digital pathology.
% \endbio

%\bio{bio-images/dummy.jpg}
%Max Mustermann is senior researcher at the awesome explainability Lab at the wonderful university of dreamland, and he is visiting researcher at the institute paradise in fantasy land. He received his Masters in computer science and her PhD in Computer Science from top university x. Erika is a member of the prestigious club wonder. 
%\endbio

% \bio{bio-images/holzinger.jpg}
% Andreas Holzinger is Visiting Professor for explainable AI at the University of Alberta, Canada since 2019 and head of the Human-Centered AI Lab at the Medical University Graz, Austria. He received his PhD in cognitive science from Graz University and his second PhD in computer science from Graz University of Technology. Andreas is ordinary member in the Academia Europaea, the European Academy of Sciences in the section Informatics, and full member of the European Lab for Learning and Intelligent Systems.
% \endbio

\end{document}


