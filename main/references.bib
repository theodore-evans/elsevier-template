% references sorted by years - newest first

% 2021 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{HolzingerEtAl:2021:GraphFusion,
   year = {2021},
   author = {Holzinger, Andreas and Malle, Bernd and Saranti, Anna and Pfeifer, Bastian},
   title = {Towards Multi-Modal Causability with Graph Neural Networks enabling Information Fusion for explainable AI},
   journal = {Information Fusion},
   volume = {71},
   number = {7},
   pages = {28--37},
   abstract = {AI is remarkably successful and outperforms human experts in certain tasks, even in complex domains such as medicine. Humans on the other hand are experts at multi-modal thinking and can embed new inputs almost instantly into a conceptual knowledge space shaped by experience. In many fields the aim is to build systems capable of explaining themselves, engaging in interactive what-if questions. Such questions, called counterfactuals, are becoming important in the rising field of explainable AI (xAI). Our central hypothesis is that using conceptual knowledge as a guiding model of reality will help to train more explainable, more robust and less biased machine learning models, ideally able to learn from fewer data. One important aspect in the medical domain is that various modalities contribute to one single result. Our main question is “How can we construct a multi-modal feature representation space (spanning images, text, genomics data) using knowledge bases as an initial connector for the development of novel explanation interface techniques?”. In this paper we argue for using Graph Neural Networks as a method-of-choice, enabling information fusion for multi-modal causability (causability – not to confuse with causality – is the measurable extent to which an explanation to a human expert achieves a specified level of causal understanding). The aim of this paper is to motivate the international xAI community to further work into the fields of multi-modal embeddings and interactive explainability, to lay the foundations for effective future human–AI interfaces. We emphasize that Graph Neural Networks play a major role for multi-modal causability, since causal links between features can be defined directly using graph structures.},
   doi = {10.1016/j.inffus.2021.01.008}
}

@article{Carrington:2021:DeepROC,
   year = {2021},
   author = {Carrington, Andre M. and Manuel, Douglas G. and Fieguth, Paul W. and Ramsay, Tim and Osmani, Venet and Wernly, Bernhard and Benett, Carol and Hawken, Steven and McInnes, Matthew and Magwood, Olivia and Sheikh, Yusuf and Holzinger, Andreas},
   title = {Deep ROC Analysis and AUC as Balanced Average Accuracy to Improve Model Selection, Understanding and Interpretation},
   journal = {arXiv:2103.11357}
}

@article{HudecEtAl-2021-Interpretable,
   year = {2021},
   author = {Hudec, Miroslav and Minarikova, Erika and Mesiar, Radko and Saranti, Anna and Holzinger, Andreas},
   title = {Classification by ordinal sums of conjunctive and disjunctive functions for explainable AI and interpretable machine learning solutions},
   journal = {Knowledge Based Systems},
   volume = {220},
   pages = {106916},
   abstract = {The main goal of classification is dividing entities into several classes. The classification considering uncertainty of belonging to the classes separates entities into the classes yes, no, maybe, where it is desirable to indicate the inclination towards belonging to yes or no. Neural networks have proven their high performance in sharp classification, but the solution is not traceable and therefore difficult or impossible for a human expert to interpret and to understand. Rule-based systems are explainable in principle, however, are based on formal inference structures and also have problems with interpretability due to their high complexity. We must stress that even human experts sometimes cannot explain, but rather construct mental models of the problem and consult these models to select the best possible solution. In our work, we propose classification by aggregation functions of the mixed behaviour by the variability in ordinal sums of conjunctive and disjunctive functions. In this way, domain experts should only assign the key observations regarding considered attributes. Consequently, the variability of functions provides room for machine learning to learn the best possible option from data. Such a solution is re-traceable, reproducible, and explainable to domain experts. In this paper we discuss the proposed approach on examples and outline the research steps in interactive machine learning with a human-in-the-loop via aggregation functions.},
   doi = {10.1016/j.knosys.2021.106916}
}


@article{piccialli_artificial_2021,
	title = {Artificial intelligence and healthcare: {Forecasting} of medical bookings through multi-source time-series fusion},
	volume = {74},
	issn = {1566-2535},
	shorttitle = {Artificial intelligence and healthcare},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253521000592},
	doi = {10.1016/j.inffus.2021.03.004},
	abstract = {Nowadays, Artificial intelligence (AI), combined with the digitalization of healthcare, can lead to substantial improvements in Patient Care, Disease Management, Hospital Administration, and supply chain effectiveness. Among predictive analytics tools, time series forecasting represents a central task to support healthcare management in terms of bookings and medical services predictions. In this context, the development of flexible frameworks to provide robust and reliable predictions became a central point in this healthcare innovation process. This paper presents and discusses a multi-source time series fusion and forecasting framework relying on Deep Learning. By combining weather, air-quality and medical bookings time series through a feature compression stage which preserves temporal patterns, the prediction is provided through a flexible ensemble technique based on machine learning models and a hybrid neural network. The proposed system is able to predict the number of bookings related to a specific medical examination for a 7-days horizon period. To assess the proposed approach’s effectiveness, we rely on time series extracted from a real dataset of administrative e-health records provided by the Campania Region health department, in Italy.},
	language = {en},
	urldate = {2021-04-22},
	journal = {Information Fusion},
	author = {Piccialli, Francesco and Giampaolo, Fabio and Prezioso, Edoardo and Camacho, David and Acampora, Giovanni},
	month = oct,
	year = {2021},
	keywords = {Artificial intelligence, Deep Learning, Healthcare, Multi-source time-series},
	pages = {1--16},
	file = {ScienceDirect Snapshot:/Users/theoevans/Zotero/storage/PIGNLZ5Y/S1566253521000592.html:text/html}
}

@article{piccialli_survey_2021,
	title = {A survey on deep learning in medicine: {Why}, how and when?},
	volume = {66},
	issn = {1566-2535},
	shorttitle = {A survey on deep learning in medicine},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253520303651},
	doi = {10.1016/j.inffus.2020.09.006},
	abstract = {New technologies are transforming medicine, and this revolution starts with data. Health data, clinical images, genome sequences, data on prescribed therapies and results obtained, data that each of us has helped to create. Although the first uses of artificial intelligence (AI) in medicine date back to the 1980s, it is only with the beginning of the new millennium that there has been an explosion of interest in this sector worldwide. We are therefore witnessing the exponential growth of health-related information with the result that traditional analysis techniques are not suitable for satisfactorily management of this vast amount of data. AI applications (especially Deep Learning), on the other hand, are naturally predisposed to cope with this explosion of data, as they always work better as the amount of training data increases, a phase necessary to build the optimal neural network for a given clinical problem. This paper proposes a comprehensive and in-depth study of Deep Learning methodologies and applications in medicine. An in-depth analysis of the literature is presented; how, where and why Deep Learning models are applied in medicine are discussed and reviewed. Finally, current challenges and future research directions are outlined and analysed.},
	language = {en},
	urldate = {2021-04-22},
	journal = {Information Fusion},
	author = {Piccialli, Francesco and Somma, Vittorio Di and Giampaolo, Fabio and Cuomo, Salvatore and Fortino, Giancarlo},
	month = feb,
	year = {2021},
	keywords = {Artificial intelligence, Data science, Deep learning, Medicine, Neural networks},
	pages = {111--137},
	file = {ScienceDirect Snapshot:/Users/theoevans/Zotero/storage/SU87876P/S1566253520303651.html:text/html}
}

@article{negahbani2021pathonet,
  title={PathoNet introduced as a deep neural network backend for evaluation of Ki-67 and tumor-infiltrating lymphocytes in breast cancer},
  author={Negahbani, Farzin and Sabzi, Rasool and Jahromi, Bita Pakniyat and Firouzabadi, Dena and Movahedi, Fateme and Shirazi, Mahsa Kohandel and Majidi, Shayan and Dehghanian, Amirreza},
  journal={Scientific reports},
  volume={11},
  number={1},
  pages={1--13},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{negahbani2020pathonet,
  title={PathoNet: Deep learning assisted evaluation of Ki-67 and tumor infiltrating lymphocytes (TILs) as prognostic factors in breast cancer; A large dataset and baseline},
  author={Negahbani, Farzin and Sabzi, Rasool and Jahromi, Bita Pakniyat and Movahedi, Fatemeh and Shirazi, Mahsa Kohandel and Majidi, Shayan and Firouzabadi, Dena and Dehganian, Amirreza},
  journal={arXiv preprint arXiv:2010.04713},
  year={2020}
}
% 2020 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{HolzingerEtAl:2020:QualityOfExplanations,
   year = {2020},
   author = {Holzinger, Andreas and Carrington, Andre and Mueller, Heimo},
   title = {Measuring the Quality of Explanations: The System Causability Scale (SCS). Comparing Human and Machine Explanations},
   journal = {KI - Kuenstliche Intelligenz (German Journal of Artificial intelligence), Special Issue on Interactive Machine Learning, Edited by Kristian Kersting, TU Darmstadt},
   volume = {34},
   number = {2},
   pages = {193--198},
   abstract = {Recent success in Artificial Intelligence (AI) and Machine Learning (ML) allow problem solving automatically without any human intervention. Autonomous approaches can be very convenient. However, in certain domains, e.g., in the medical domain, it is necessary to enable a domain expert to understand, why an algorithm came up with a certain result. Consequently, the field of Explainable AI (xAI) rapidly gained interest worldwide in various domains, particularly in medicine. Explainable AI studies transparency and traceability of opaque AI/ML and there are already a huge variety of methods. For example with layer-wise relevance propagation relevant parts of inputs to, and representations in, a neural network which caused a result, can be highlighted. This is a first important step to ensure that end users, e.g., medical professionals, assume responsibility for decision making with AI/ML and of interest to professionals and regulators. Interactive ML adds the component of human expertise to AI/ML processes by enabling them to re-enact and retrace AI/ML results, e.g. let them check it for plausibility. This requires new human-AI interfaces for explainable AI. In order to build effective and efficient interactive human-AI interfaces we have to deal with the question of how to evaluate the quality of explanations given by an explainable AI system. In this paper we introduce our System Causability Scale (SCS) to measure the quality of explanations. It is based on our notion of Causability (Holzinger et al., 2019) combined with concepts adapted from a widely accepted usability scale.},
   doi = {10.1007/s13218-020-00636-z}
}

@incollection{poceviciute_survey_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Survey of {XAI} in {Digital} {Pathology}},
	isbn = {978-3-030-50402-1},
	url = {https://doi.org/10.1007/978-3-030-50402-1_4},
	abstract = {Artificial intelligence (AI) has shown great promise for diagnostic imaging assessments. However, the application of AI to support medical diagnostics in clinical routine comes with many challenges. The algorithms should have high prediction accuracy but also be transparent, understandable and reliable. Thus, explainable artificial intelligence (XAI) is highly relevant for this domain. We present a survey on XAI within digital pathology, a medical imaging sub-discipline with particular characteristics and needs. The review includes several contributions. Firstly, we give a thorough overview of current XAI techniques of potential relevance for deep learning methods in pathology imaging, and categorise them from three different aspects. In doing so, we incorporate uncertainty estimation methods as an integral part of the XAI landscape. We also connect the technical methods to the specific prerequisites in digital pathology and present findings to guide future research efforts. The survey is intended for both technical researchers and medical professionals, one of the objectives being to establish a common ground for cross-disciplinary discussions.},
	language = {en},
	urldate = {2020-08-05},
	booktitle = {Artificial {Intelligence} and {Machine} {Learning} for {Digital} {Pathology}: {State}-of-the-{Art} and {Future} {Challenges}},
	publisher = {Springer International Publishing},
	author = {Pocevičiūtė, Milda and Eilertsen, Gabriel and Lundström, Claes},
	editor = {Holzinger, Andreas and Goebel, Randy and Mengel, Michael and Müller, Heimo},
	year = {2020},
	doi = {10.1007/978-3-030-50402-1_4},
	keywords = {Digital pathology, AI, Medical imaging, XAI},
	pages = {56--88},
	file = {Springer Full Text PDF:/Users/theoevans/Zotero/storage/TD83KLSG/Pocevičiūtė et al. - 2020 - Survey of XAI in Digital Pathology.pdf:application/pdf}
}

@book{holzinger_artificial_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Artificial {Intelligence} and {Machine} {Learning} for {Digital} {Pathology}: {State}-of-the-{Art} and {Future} {Challenges}},
	volume = {12090},
	isbn = {978-3-030-50401-4 978-3-030-50402-1},
	shorttitle = {Artificial {Intelligence} and {Machine} {Learning} for {Digital} {Pathology}},
	url = {http://link.springer.com/10.1007/978-3-030-50402-1},
	language = {en},
	urldate = {2020-08-05},
	publisher = {Springer International Publishing},
	editor = {Holzinger, Andreas and Goebel, Randy and Mengel, Michael and Müller, Heimo},
	year = {2020},
	doi = {10.1007/978-3-030-50402-1},
	file = {Holzinger et al. - 2020 - Artificial Intelligence and Machine Learning for D.pdf:/Users/theoevans/Zotero/storage/T42DPBRD/Holzinger et al. - 2020 - Artificial Intelligence and Machine Learning for D.pdf:application/pdf}
}

@incollection{regitnig_expectations_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Expectations of {Artificial} {Intelligence} for {Pathology}},
	isbn = {978-3-030-50402-1},
	url = {https://doi.org/10.1007/978-3-030-50402-1_1},
	abstract = {Within the last ten years, essential steps have been made to bring artificial intelligence (AI) successfully into the field of pathology. However, most pathologists are still far away from using AI in daily pathology practice. If one leaves the pathology annihilation model, this paper focuses on tasks, which could be solved, and which could be done better by AI, or image-based algorithms, compared to a human expert. In particular, this paper focuses on the needs and demands of surgical pathologists; examples include: Finding small tumour deposits within lymph nodes, detection and grading of cancer, quantification of positive tumour cells in immunohistochemistry, pre-check of Papanicolaou-stained gynaecological cytology in cervical cancer screening, text feature extraction, text interpretation for tumour-coding error prevention and AI in the next-generation virtual autopsy. However, in order to make substantial progress in both fields it is important to intensify the cooperation between medical AI experts and pathologists.},
	language = {en},
	urldate = {2020-08-11},
	booktitle = {Artificial {Intelligence} and {Machine} {Learning} for {Digital} {Pathology}: {State}-of-the-{Art} and {Future} {Challenges}},
	publisher = {Springer International Publishing},
	author = {Regitnig, Peter and Müller, Heimo and Holzinger, Andreas},
	editor = {Holzinger, Andreas and Goebel, Randy and Mengel, Michael and Müller, Heimo},
	year = {2020},
	doi = {10.1007/978-3-030-50402-1_1},
	keywords = {Machine learning, Digital pathology, Artificial Intelligence, Medical AI, Practical implementations of AI},
	pages = {1--15},
	file = {Springer Full Text PDF:/Users/theoevans/Zotero/storage/8IVFNTBJ/Regitnig et al. - 2020 - Expectations of Artificial Intelligence for Pathol.pdf:application/pdf}
}

@article{zhou_comprehensive_2020,
	title = {A {Comprehensive} {Review} for {Breast} {Histopathology} {Image} {Analysis} {Using} {Classical} and {Deep} {Neural} {Networks}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2993788},
	abstract = {Breast cancer is one of the most common and deadliest cancers among women. Since histopathological images contain sufficient phenotypic information, they play an indispensable role in the diagnosis and treatment of breast cancers. To improve the accuracy and objectivity of Breast Histopathological Image Analysis (BHIA), Artificial Neural Network (ANN) approaches are widely used in the segmentation and classification tasks of breast histopathological images. In this review, we present a comprehensive overview of the BHIA techniques based on ANNs. First of all, we categorize the BHIA systems into classical and deep neural networks for in-depth investigation. Then, the relevant studies based on BHIA systems are presented. After that, we analyze the existing models to discover the most suitable algorithms. Finally, publicly accessible datasets, along with their download links, are provided for the convenience of future researchers.},
	journal = {IEEE Access},
	author = {Zhou, X. and Li, C. and Rahaman, M. M. and Yao, Y. and Ai, S. and Sun, C. and Wang, Q. and Zhang, Y. and Li, M. and Li, X. and Jiang, T. and Xue, D. and Qi, S. and Teng, Y.},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Machine learning, deep learning, Breast cancer, convolutional neural networks, Image analysis, Neural networks, image segmentation, histopathology, image classification},
	pages = {90931--90956},
	file = {IEEE Xplore Abstract Record:/Users/theoevans/Zotero/storage/QAV3DP67/9091012.html:text/html;IEEE Xplore Full Text PDF:/Users/theoevans/Zotero/storage/3V5YFV4R/Zhou et al. - 2020 - A Comprehensive Review for Breast Histopathology I.pdf:application/pdf}
}

@article{tjoa_survey_2020,
	title = {A {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI}): {Toward} {Medical} {XAI}},
	issn = {2162-2388},
	shorttitle = {A {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	doi = {10.1109/TNNLS.2020.3027314},
	abstract = {Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Tjoa, E. and Guan, C.},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {interpretability, Visualization, Biomedical imaging, Prediction algorithms, Artificial neural networks, Explainable artificial intelligence (XAI), Heating systems, Learning systems, machine learning (ML), medical information system, Reliability, survey.},
	pages = {1--21},
	file = {IEEE Xplore Abstract Record:/Users/theoevans/Zotero/storage/VUA2GC6J/9233366.html:text/html;IEEE Xplore Full Text PDF:/Users/theoevans/Zotero/storage/YFBV76SS/Tjoa and Guan - 2020 - A Survey on Explainable Artificial Intelligence (X.pdf:application/pdf}
}

@inproceedings{chakraborti_emerging_2020,
	title = {The {Emerging} {Landscape} of {Explainable} {Automated} {Planning} \&amp; {Decision} {Making}},
	volume = {5},
	url = {https://www.ijcai.org/proceedings/2020/669},
	doi = {10.24963/ijcai.2020/669},
	abstract = {Electronic proceedings of IJCAI 2020},
	language = {en},
	urldate = {2021-04-27},
	author = {Chakraborti, Tathagata and Sreedharan, Sarath and Kambhampati, Subbarao},
	month = jul,
	year = {2020},
	note = {ISSN: 1045-0823},
	pages = {4803--4811},
	file = {Full Text PDF:/Users/theoevans/Zotero/storage/35CU5I2I/Chakraborti et al. - 2020 - The Emerging Landscape of Explainable Automated Pl.pdf:application/pdf}
}


% 2019 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{HolzingerEtAl:2019:Wiley-Paper,
   year = {2019},
   author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and Mueller, Heimo},
   title = {Causability and Explainability of Artificial Intelligence in Medicine},
   journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
   volume = {9},
   number = {4},
   pages = {1--13},
   abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black‐box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use‐case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system.},
   doi = {10.1002/widm.1312}
}

@article{Holzinger:2019:HumanLoopAPIN,
   year = {2019},
   author = {Holzinger, Andreas and Plass, Markus and Kickmeier-Rust, Michael and Holzinger, Katharina and Crisan, Gloria Cerasela and Pintea, Camelia-M. and Palade, Vasile},
   title = {Interactive machine learning: experimental evidence for the human in the algorithmic loop},
   journal = {Applied Intelligence},
   volume = {49},
   number = {7},
   pages = {2401--2414},
   abstract = {Recent advances in automatic machine learning (aML) allow solving problems without any human intervention. However, sometimes a human-in-the-loop can be beneficial in solving computationally hard problems. In this paper we provide new experimental insights on how we can improve computational intelligence by complementing it with human intelligence in an interactive machine learning approach (iML). For this purpose, we used the Ant Colony Optimization (ACO) framework, because this fosters multi-agent approaches with human agents in the loop. We propose unification between the human intelligence and interaction skills and the computational power of an artificial system. The ACO framework is used on a case study solving the Traveling Salesman Problem, because of its many practical implications, e.g. in the medical domain. We used ACO due to the fact that it is one of the best algorithms used in many applied intelligence problems. For the evaluation we used gamification, i.e. we implemented a snake-like game called Traveling Snakesman with the MAX–MIN Ant System (MMAS) in the background. We extended the MMAS–Algorithm in a way, that the human can directly interact and influence the ants. This is done by “traveling” with the snake across the graph. Each time the human travels over an ant, the current pheromone value of the edge is multiplied by 5. This manipulation has an impact on the ant’s behavior (the probability that this edge is taken by the ant increases). The results show that the humans performing one tour through the graphs have a significant impact on the shortest path found by the MMAS. Consequently, our experiment demonstrates that in our case human intelligence can positively influence machine intelligence. To the best of our knowledge this is the first study of this kind.},
   doi = {10.1007/s10489-018-1361-5}
}

% 2018 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@incollection{Holzinger:2018:IEEE-DISA,
   year = {2018},
   author = {Holzinger, Andreas},
   title = {From Machine Learning to Explainable AI},
   booktitle = {2018 World Symposium on Digital Intelligence for Systems and Machines (IEEE DISA)},
   publisher = {IEEE},
   pages = {55--66},
   abstract = {The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible “glass-box” approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.},
   doi = {10.1109/DISA.2018.8490530}
}



% 2017 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@incollection{SinghEtAl:2017:RNN-AAL,
   year = {2017},
   author = {Singh, Deepika and Merdivan, Erinc and Psychoula, Ismini and Kropf, Johannes and Hanke, Sten and Geist, Matthieu and Holzinger, Andreas},
   title = {Human Activity Recognition Using Recurrent Neural Networks},
   booktitle = {Machine Learning and Knowledge Extraction, CD-MAKE, Lecture Notes in Computer Science LNCS 10410},
   publisher = {Springer International},
   address = {Cham},
   pages = {267--274},
   abstract = {Human activity recognition using smart home sensors is one of the bases of ubiquitous computing in smart environments and a topic undergoing intense research in the field of ambient assisted living. The increasingly large amount of data sets calls for machine learning methods. In this paper, we introduce a deep learning model that learns to classify human activities without using any prior knowledge. For this purpose, a Long Short Term Memory (LSTM) Recurrent Neural Network was applied to three real world smart home datasets. The results of these experiments show that the proposed approach outperforms the existing ones in terms of accuracy and performance.},
   doi = {10.1007/978-3-319-66808-6-8}
}


@article{holzinger_what_2017,
	title = {What do we need to build explainable {AI} systems for the medical domain?},
	url = {http://arxiv.org/abs/1712.09923},
	abstract = {Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.},
	urldate = {2020-06-14},
	journal = {arXiv:1712.09923 [cs, stat]},
	author = {Holzinger, Andreas and Biemann, Chris and Pattichis, Constantinos S. and Kell, Douglas B.},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.09923},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: This is a survey article and section 3.1. draws heavily from arXiv:1706.07979},
	file = {arXiv Fulltext PDF:/Users/theoevans/Zotero/storage/R9ZYXJ2W/Holzinger et al. - 2017 - What do we need to build explainable AI systems fo.pdf:application/pdf;arXiv.org Snapshot:/Users/theoevans/Zotero/storage/H2NZK9AG/1712.html:text/html}
}

@article{holzinger_towards_2017,
	title = {Towards the {Augmented} {Pathologist}: {Challenges} of {Explainable}-{AI} in {Digital} {Pathology}},
	shorttitle = {Towards the {Augmented} {Pathologist}},
	url = {http://arxiv.org/abs/1712.06657},
	abstract = {Digital pathology is not only one of the most promising fields of diagnostic medicine, but at the same time a hot topic for fundamental research. Digital pathology is not just the transfer of histopathological slides into digital representations. The combination of different data sources (images, patient records, and *omics data) together with current advances in artificial intelligence/machine learning enable to make novel information accessible and quantifiable to a human expert, which is not yet available and not exploited in current medical settings. The grand goal is to reach a level of usable intelligence to understand the data in the context of an application task, thereby making machine decisions transparent, interpretable and explainable. The foundation of such an "augmented pathologist" needs an integrated approach: While machine learning algorithms require many thousands of training examples, a human expert is often confronted with only a few data points. Interestingly, humans can learn from such few examples and are able to instantly interpret complex patterns. Consequently, the grand goal is to combine the possibilities of artificial intelligence with human intelligence and to find a well-suited balance between them to enable what neither of them could do on their own. This can raise the quality of education, diagnosis, prognosis and prediction of cancer and other diseases. In this paper we describe some (incomplete) research issues which we believe should be addressed in an integrated and concerted effort for paving the way towards the augmented pathologist.},
	urldate = {2020-07-31},
	journal = {arXiv:1712.06657 [cs, stat]},
	author = {Holzinger, Andreas and Malle, Bernd and Kieseberg, Peter and Roth, Peter M. and Müller, Heimo and Reihs, Robert and Zatloukal, Kurt},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.06657},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/theoevans/Zotero/storage/QRQ6UU4T/Holzinger et al. - 2017 - Towards the Augmented Pathologist Challenges of E.pdf:application/pdf;arXiv.org Snapshot:/Users/theoevans/Zotero/storage/K4XZ6R9I/1712.html:text/html}
}

% 2016 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Holzinger:2016:iML,
   year = {2016},
   author = {Holzinger, Andreas},
   title = {Interactive Machine Learning for Health Informatics: When do we need the human-in-the-loop?},
   journal = {Brain Informatics},
   volume = {3},
   number = {2},
   pages = {119--131},
   abstract = {Machine learning (ML) is the fastest growing field in computer science, and health informatics is amongst the greatest challenges. The goal of ML is to develop algorithms which can learn and improve over time and can be used for predictions. Most ML researchers concentrate on automatic Machine Learning (aML), where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from big data with many training sets. However, in the health domain, sometimes we are confronted with a small number of data sets or rare events, where aML-approaches suffer of insufficient training samples. Here interactive Machine Learning (iML) may be of help, having its roots in Reinforcement Learning (RL), Preference Learning (PL) and Active Learning (AL). The term iML is not yet well used, so we define it as algorithms that can interact with agents and can optimize their learning behaviour through these interactions, where the agents can also be human. This human-in-the-loop can be beneficial in solving computationally hard problems, e.g., subspace clustering, protein folding, or k-anonymization of health data, where human expertise can help to reduce an exponential search space through heuristic selection of samples. Therefore, what would otherwise be an NP-hard problem, reduces greatly in complexity through the input and the assistance of a human agent involved in the learning phase.},
   keywords = {interactive Machine learning, health informatics},
   doi = {10.1007/s40708-016-0042-6}
}






